{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a7fd45-280a-4d01-99b8-5b8e37b688fe",
   "metadata": {},
   "source": [
    "# Ice skating talent workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc5af0-f268-4970-a2df-d4f70e257d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leave your information so we may contact you\n",
    "## These notebooks are deleted a few days after the workshop\n",
    "## Feel free to only fill in the fields you want\n",
    "YOUR_INFORMATION = dict(\n",
    "    name=\"Patrick\",\n",
    "    phone_number=\"+31612324087\",\n",
    "    email=\"patrickacamara@gmail.com\",\n",
    "    feedback=\"your workshop feedback to us <3\",\n",
    "    study=\"AI\",\n",
    "    linkedin=\"https://www.linkedin.com/in/patrick-camara/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e7328-2f1d-499e-bcb0-fc7fb22d040b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this workshop is to develop a machine learning tool that can **predict the performance of ice skaters**. Such a tool could be used to help identify the biggest talents in the field and aid coaches in selecting athletes for training and competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e8b68-fc2f-4698-8517-0a0934d4fd02",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "First we set up our work environment. We use a bunch of packages and load our prepared datasets.\n",
    "\n",
    "When you make changes to the notebook, it is often best to use the `Run All >>` button at the top. We've done our best to make sure running the entire notebook is quite fast.\n",
    "\n",
    "## Shortcuts\n",
    "\n",
    "- `Shift+Enter` to execute a cell - you can spam this to continue running cells\n",
    "- In the left sidebar, the third entry is a `Navigation` tab. Use it to quickly navigate the notebooks.\n",
    "- Drag the tab of a notebook to split the view\n",
    "- In the top-bar -> Settings -> Theme -> Jupyter Dark\n",
    "\n",
    "### Tools\n",
    "\n",
    "We import some specific packages for this workshop:\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/docs/index.html):  A widely used data structures and data analysis package for processing our data as DataFrames;\n",
    "- [Jupyter](https://jupyter.org/):  The workspace you are working in right now, to easily create notebooks for experimenting with data, models, and visualisations;\n",
    "- [scikit-learn](https://scikit-learn.org/stable/index.html ):  To create model pipelines combining transformers and estimators;\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html): A widely used prediction model for both classification and regression that often needs little tuning to quickly get decent results;\n",
    "\n",
    "\n",
    "Of course you are free to use other machine learning libraries as well, for example PyTorch or Tensorflow,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf854a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd8d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "Qualogy manages the historical database for the KNSB. For every skater we have historical records about all races they participated in, as well as data about the skater like their nationality, gender and age.\n",
    "\n",
    "#### Features\n",
    "\n",
    "Our **features** should capture the historical performance of a skater. To summarize the **history of a skater at a given age**, we take all their data from before that age and summarize it. Our goal is to create features such that we have **1 record for person_id + season_age**. To prevent data-leakage, we may not use any data of the year we are trying to predict the performance for.\n",
    "\n",
    "#### Target\n",
    "\n",
    "Our **target**s are the **future season_best records**, so we can predict how good a skater will be in the future - given his performance until the current age. For every age we predict multiple years ahead.\n",
    "\n",
    "### Feature Engineering\n",
    "The data we have prepared for you contains both static features and timeseries features.\n",
    " - **Static features**: data that does not change over time, such as [`birthdates`, `name`, `gender`]\n",
    " - **Timeseries features**: this data changes over time, and is aggregated by yearly seasons.\n",
    "   - Aggregations over the historical data of the skaters, which includes but not limited to:\n",
    "       - `season_best` to represent the performance for every yearly season\n",
    "       - `n_races_season` for the number of races participated in during a season\n",
    "   - Summary of the performance of the previous years of an individual skater.\n",
    "   - This is what is shown in the figure below.\n",
    "\n",
    "![image.png](https://tsfresh.readthedocs.io/en/latest/_images/rolling_mechanism_1.png)!\n",
    "![image.png](https://tsfresh.readthedocs.io/en/latest/_images/rolling_mechanism_2.png)!\n",
    "\n",
    "In the figures above, the contributors to [TSFresh](https://tsfresh.readthedocs.io/en/latest/text/forecasting.html) illustrate how we calculate rolling features to predict a target. In this illustration **only one-step ahead targets are shown**. For this workshop, **we create targets for ALL future records**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081ff9b-dbf9-4404-b128-311daa8bda75",
   "metadata": {},
   "source": [
    "## Get the train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2218a340-bed9-4889-b3d5-3e348f66fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   person_id  season_age  season_age_target  personal_best  season_best  \\\n",
      "0       3681          14                 15          48.12        48.12   \n",
      "1       3681          14                 16          48.12        48.12   \n",
      "2       3681          14                 17          48.12        48.12   \n",
      "3       3681          14                 18          48.12        48.12   \n",
      "4       3681          14                 19          48.12        48.12   \n",
      "\n",
      "   std_season_best  std_season_best_3_years  season_avg_time  \\\n",
      "0              NaN                      NaN           50.915   \n",
      "1              NaN                      NaN           50.915   \n",
      "2              NaN                      NaN           50.915   \n",
      "3              NaN                      NaN           50.915   \n",
      "4              NaN                      NaN           50.915   \n",
      "\n",
      "   1_year_improvement  2_year_improvement  ...  \\\n",
      "0                 NaN                 NaN  ...   \n",
      "1                 NaN                 NaN  ...   \n",
      "2                 NaN                 NaN  ...   \n",
      "3                 NaN                 NaN  ...   \n",
      "4                 NaN                 NaN  ...   \n",
      "\n",
      "   total_races_world_junior_championships  total_races_junior_world_cups  \\\n",
      "0                                     0.0                            0.0   \n",
      "1                                     0.0                            0.0   \n",
      "2                                     0.0                            0.0   \n",
      "3                                     0.0                            0.0   \n",
      "4                                     0.0                            0.0   \n",
      "\n",
      "   total_races_world_cups  season_races_national_junior_championships  \\\n",
      "0                     0.0                                         0.0   \n",
      "1                     0.0                                         0.0   \n",
      "2                     0.0                                         0.0   \n",
      "3                     0.0                                         0.0   \n",
      "4                     0.0                                         0.0   \n",
      "\n",
      "   season_races_world_junior_championships  season_races_junior_world_cups  \\\n",
      "0                                      0.0                             0.0   \n",
      "1                                      0.0                             0.0   \n",
      "2                                      0.0                             0.0   \n",
      "3                                      0.0                             0.0   \n",
      "4                                      0.0                             0.0   \n",
      "\n",
      "   season_races_world_cups  gender  nationality  year_of_birth  \n",
      "0                      0.0       M       Canada           1987  \n",
      "1                      0.0       M       Canada           1987  \n",
      "2                      0.0       M       Canada           1987  \n",
      "3                      0.0       M       Canada           1987  \n",
      "4                      0.0       M       Canada           1987  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "0        46.840\n",
      "1        42.130\n",
      "2        39.520\n",
      "3        38.210\n",
      "4        37.520\n",
      "          ...  \n",
      "27327    48.012\n",
      "27328    49.298\n",
      "27329    48.012\n",
      "27330    49.298\n",
      "27331    49.298\n",
      "Name: season_best, Length: 22193, dtype: float64\n",
      "person_id                                        0\n",
      "season_age                                       0\n",
      "season_age_target                                0\n",
      "personal_best                                    0\n",
      "season_best                                      0\n",
      "std_season_best                               1019\n",
      "std_season_best_3_years                       1019\n",
      "season_avg_time                                  0\n",
      "1_year_improvement                            1019\n",
      "2_year_improvement                            2726\n",
      "season                                           0\n",
      "n_races_season                                   0\n",
      "n_races_total                                    0\n",
      "n_races_3_year                                   0\n",
      "years_of_experience                              0\n",
      "total_races_national_junior_championships        0\n",
      "total_races_world_junior_championships           0\n",
      "total_races_junior_world_cups                    0\n",
      "total_races_world_cups                           0\n",
      "season_races_national_junior_championships       0\n",
      "season_races_world_junior_championships          0\n",
      "season_races_junior_world_cups                   0\n",
      "season_races_world_cups                          0\n",
      "gender                                           0\n",
      "nationality                                      0\n",
      "year_of_birth                                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_pickle(\"data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"data/y_test.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "nan_count_col1 = X_train.isna().sum()\n",
    "print(nan_count_col1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afb1a9-e511-4a53-be6d-470da23550fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(y_train)\n",
    "\n",
    "X_train.describe()\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998408d4",
   "metadata": {},
   "source": [
    "## Display data\n",
    "\n",
    "At every prediction moment, we create features based on all the known information until that point. Then we use those features to predict all known future `season_best` values. Of course, these values are only known in our training data.\n",
    "\n",
    "The real use of this model is to apply it to current young talent to predict their unkown future performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf513d0",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "The **FUN** part!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd244c-dd7e-46d7-b4f8-a9400025003d",
   "metadata": {},
   "source": [
    "### 1. Create scikit-learn transformers\n",
    "Can must instantiate a transformer from scikit-learn before using it in the model.\n",
    "Here we create some transformers for you to start with. \n",
    "\n",
    "We want you to experiment with the Transformers, and their hyperparameters.\n",
    "You can add any other transformers if you wish: https://scikit-learn.org/stable/data_transforms.html\n",
    "\n",
    "### 2. Create the estimator using regression\n",
    "\n",
    "### 3. Create your pipeline by combining transformers and your estimator\n",
    "A model consists of a pipeline of Transformers that are performed sequentially and a final step that is an Estimator (xgboost in this case).\n",
    " - Add/remove more transformers by adding a tuple of ('name', transformer_object) or commenting out any of the steps below;\n",
    " - You can also change the XGBRegressor to use a different estimator but for this workshop we do not recommend it.\n",
    "\n",
    "\n",
    "### 4. (Alternative) Build a machine learning model with PyTorch / Tensorflow.\n",
    "Instead of using regression models, you are free to create a different approach, such as building a neural network ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd91dcb",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "**Here we want you to edit the cells below to change and experiment with your model pipeline.**\n",
    "\n",
    "Sklearn docs: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2fe673-e292-405a-9fe1-cdeaea83e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person_id' 'season_age' 'season_age_target' 'personal_best'\n",
      " 'season_best' 'std_season_best' 'std_season_best_3_years'\n",
      " 'season_avg_time' '1_year_improvement' '2_year_improvement' 'season'\n",
      " 'n_races_season' 'n_races_total' 'n_races_3_year' 'years_of_experience'\n",
      " 'total_races_national_junior_championships'\n",
      " 'total_races_world_junior_championships' 'total_races_junior_world_cups'\n",
      " 'total_races_world_cups' 'season_races_national_junior_championships'\n",
      " 'season_races_world_junior_championships'\n",
      " 'season_races_junior_world_cups' 'season_races_world_cups' 'gender'\n",
      " 'nationality' 'year_of_birth']\n"
     ]
    }
   ],
   "source": [
    "# Experiment and create your own model.\n",
    "import sklearn\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import plot_importance, XGBRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# You can group the prediction features into various lists to run different\n",
    "# This helps us to treat different types of variables differently.\n",
    "# Feel free to make changes to this list by including/excluding other columns from `X_train`.\n",
    "\n",
    "all_features = X_train.columns.values\n",
    "\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f4cf2-e5e3-4a96-9f14-099eede8754d",
   "metadata": {},
   "source": [
    "## Create model pipeline\n",
    "\n",
    "The following cell shows a baseline model to predict the ice skaters performance in future years.\n",
    "You can make changes to the pipeline, for example swapping the estimator or adding/removing steps in the pipeline.\n",
    "\n",
    "Edit the cells below to create a new model pipeline.\n",
    "\n",
    "- Pick / Divide the features;\n",
    "- Add / Remove / Combine transformers;\n",
    "- Make your final step an `Estimator`; such as LinearRegression or an XGBoost Regressor\n",
    "- For each transformer you can tweak the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e19bf2e-0c20-47f0-aad3-f575ebe6a7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;pipeline_1&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler()),\n",
       "                                                                  (&#x27;pca&#x27;,\n",
       "                                                                   PCA(n_components=5))]),\n",
       "                                                  [&#x27;season_age&#x27;,\n",
       "                                                   &#x27;season_age_target&#x27;,\n",
       "                                                   &#x27;season_best&#x27;,\n",
       "                                                   &#x27;year_of_birth&#x27;,\n",
       "                                                   &#x27;n_races_season&#x27;]),\n",
       "                                                 (&#x27;pipeline_2&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                                                   StandardScaler(with_std=False)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;year_of_birth&#x27;,\n",
       "                                                   &#x27;years_of_experience&#x27;])])),\n",
       "                (&#x27;estimator&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;pipeline_1&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   MinMaxScaler()),\n",
       "                                                                  (&#x27;pca&#x27;,\n",
       "                                                                   PCA(n_components=5))]),\n",
       "                                                  [&#x27;season_age&#x27;,\n",
       "                                                   &#x27;season_age_target&#x27;,\n",
       "                                                   &#x27;season_best&#x27;,\n",
       "                                                   &#x27;year_of_birth&#x27;,\n",
       "                                                   &#x27;n_races_season&#x27;]),\n",
       "                                                 (&#x27;pipeline_2&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                                                   StandardScaler(with_std=False)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=&#x27;constant&#x27;))]),\n",
       "                                                  [&#x27;year_of_birth&#x27;,\n",
       "                                                   &#x27;years_of_experience&#x27;])])),\n",
       "                (&#x27;estimator&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;pipeline_1&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                                                 (&#x27;pca&#x27;, PCA(n_components=5))]),\n",
       "                                 [&#x27;season_age&#x27;, &#x27;season_age_target&#x27;,\n",
       "                                  &#x27;season_best&#x27;, &#x27;year_of_birth&#x27;,\n",
       "                                  &#x27;n_races_season&#x27;]),\n",
       "                                (&#x27;pipeline_2&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;standard_scaler&#x27;,\n",
       "                                                  StandardScaler(with_std=False)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(fill_value=0,\n",
       "                                                                strategy=&#x27;constant&#x27;))]),\n",
       "                                 [&#x27;year_of_birth&#x27;, &#x27;years_of_experience&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline_1</label><div class=\"sk-toggleable__content\"><pre>[&#x27;season_age&#x27;, &#x27;season_age_target&#x27;, &#x27;season_best&#x27;, &#x27;year_of_birth&#x27;, &#x27;n_races_season&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=5)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline_2</label><div class=\"sk-toggleable__content\"><pre>[&#x27;year_of_birth&#x27;, &#x27;years_of_experience&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_std=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('column_transformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline_1',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   MinMaxScaler()),\n",
       "                                                                  ('pca',\n",
       "                                                                   PCA(n_components=5))]),\n",
       "                                                  ['season_age',\n",
       "                                                   'season_age_target',\n",
       "                                                   'season_best',\n",
       "                                                   'year_of_birth',\n",
       "                                                   'n_races_season']),\n",
       "                                                 ('pipeline_2',\n",
       "                                                  Pipeline(steps=[('standard_scaler',\n",
       "                                                                   StandardScaler(with_std=False)),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['year_of_birth',\n",
       "                                                   'years_of_experience'])])),\n",
       "                ('estimator', LinearRegression())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # We selected a few features to train the model on\n",
    "# subset_features_1 = [\n",
    "#     'season_age',\n",
    "#     'season_age_target',\n",
    "#     'season_best',\n",
    "#     'year_of_birth',\n",
    "#     'n_races_season',\n",
    "# ]\n",
    "\n",
    "# subset_features_2 = [\n",
    "#     'year_of_birth',\n",
    "#     'years_of_experience',\n",
    "# ]\n",
    "\n",
    "# # two simple pipelines, that use the features from above\n",
    "# pipeline1 = Pipeline([\n",
    "#     (\"scaler\", MinMaxScaler()),\n",
    "#     (\"pca\", PCA(n_components=5))\n",
    "# ])\n",
    "\n",
    "# pipeline2 = Pipeline([\n",
    "#     (\"standard_scaler\", StandardScaler(with_mean=True, with_std=False)),\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=False)),\n",
    "# ])\n",
    "\n",
    "# # An estimator as example\n",
    "# estimator = LinearRegression()\n",
    "\n",
    "# # complete the pipeline by adding all steps together\n",
    "# model = Pipeline([\n",
    "#     (\"column_transformer\", ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"pipeline_1\", pipeline1, subset_features_1),\n",
    "#             (\"pipeline_2\", pipeline2, subset_features_2),\n",
    "#         ]\n",
    "#     )),\n",
    "#     (\"estimator\", estimator)\n",
    "# ])\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cab6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train models\n",
    "\n",
    "- Train the model by calling `model.fit(X_train, y_train)`;\n",
    "- Predict with the model by calling `model.predict(X_test)`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bdd9bfd-979d-42d4-ac48-c4d44c3fabe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation of Model Setup and Decisions:\n",
    "\n",
    "1. Feature Selection:\n",
    "   - 'subset_features_1' focuses on direct performance metrics and improvements:\n",
    "     'personal_best' and 'season_best' are crucial as they directly indicate the skater's best times.\n",
    "     'std_season_best' and 'std_season_best_3_years' provide insights into the variability of performances, \n",
    "     which can help understand consistency and potential under varying conditions.\n",
    "     'season_avg_time', '1_year_improvement', and '2_year_improvement' are included to capture recent trends \n",
    "     and changes in performance, offering predictive clues about future improvements or regressions.\n",
    "\n",
    "   - 'subset_features_2' combines experience and demographic features:\n",
    "     'years_of_experience' and 'n_races_total', 'n_races_3_year' quantify the amount of competitive exposure \n",
    "     a skater has, which is often correlated with their skill level and adaptability.\n",
    "     'gender' and 'nationality' are included to potentially capture demographic influences on performance,\n",
    "     acknowledging that certain trends might exist within different groups or national sports systems.\n",
    "\n",
    "2. Preprocessing Pipelines:\n",
    "   - The 'numerical_pipeline' includes:\n",
    "     'SimpleImputer' with a strategy of 'mean' to handle missing values in numerical data by replacing them with \n",
    "     the mean of their respective columns, ensuring no data point is discarded due to missing values.\n",
    "     'StandardScaler' normalizes these features to have zero mean and unit variance, which is important for models \n",
    "     like PCA and many machine learning algorithms to perform optimally.\n",
    "     'PCA' is used to reduce the dimensionality of the data to the three most informative components, simplifying \n",
    "     the model while retaining the most significant information, which helps in improving model performance and \n",
    "     reducing overfitting.\n",
    "\n",
    "   - The 'categorical_pipeline' includes:\n",
    "     'SimpleImputer' with a strategy of 'most frequent' to fill missing values in categorical data with the most \n",
    "     common category, preserving the statistical properties of the feature.\n",
    "     'OneHotEncoder' transforms categorical variables into a numerical format that the model can utilize, \n",
    "     expanding the feature space to include binary variables that represent the presence or absence of each category.\n",
    "\n",
    "3. Column Transformer:\n",
    "   - This tool applies the specified 'numerical_pipeline' to 'subset_features_1' and 'categorical_pipeline' to \n",
    "     'subset_features_2', ensuring that each feature subset is processed according to its nature and requirements.\n",
    "     The 'remainder' parameter set to 'passthrough' allows any other features not explicitly mentioned to still \n",
    "     be included in the model without alteration, providing flexibility.\n",
    "\n",
    "4. Model Pipeline:\n",
    "   - Integrates preprocessing and modeling steps into a single pipeline, streamlining the process from raw data \n",
    "     to predictions. This setup uses 'RandomForestRegressor', a robust and versatile ensemble method that can handle \n",
    "     both linear and non-linear relationships effectively. It is chosen for its ability to model complex interactions \n",
    "     between features and its general robustness to overfitting and various data imperfections.\n",
    "\n",
    "5. Model Training and Evaluation:\n",
    "   - The model is trained using the processed features from 'X_train' and the target 'y_train'. After training, \n",
    "     it is evaluated on 'X_test' to calculate the Mean Squared Error (MSE) from the predictions compared to 'y_test'.\n",
    "     This MSE provides a quantitative measure of the model's prediction accuracy in the same units as the target variable.\n",
    "\n",
    "This setup is designed to be comprehensive yet efficient, ensuring that each type of data is handled appropriately \n",
    "and that the model benefits from both raw performance data and contextual demographic information to provide accurate \n",
    "and reliable predictions.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d902867-7c06-4202-9c5c-df8717ee10fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.647088689489535\n"
     ]
    }
   ],
   "source": [
    "subset_features_1 = [\n",
    "    'personal_best', 'season_best', 'std_season_best', 'std_season_best_3_years',\n",
    "    'season_avg_time', '1_year_improvement', '2_year_improvement'\n",
    "]\n",
    "\n",
    "subset_features_2 = [\n",
    "    'years_of_experience', 'n_races_total', 'n_races_3_year',\n",
    "    'gender', 'nationality'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Define more comprehensive preprocessing for numerical and categorical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Impute missing values with the mean of the column\n",
    "    (\"scaler\", StandardScaler()),  # Scale data to have zero mean and unit variance\n",
    "    (\"pca\", PCA(n_components=3))   # Reduce dimensionality to focus on the most informative aspects\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values with the most frequent category\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown='ignore'))  # Convert categorical variables into a format that can be provided to the model\n",
    "])\n",
    "\n",
    "# Define column transformer to apply different preprocessing to different subsets of data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, subset_features_1),  # Apply numerical_pipeline to first subset of features\n",
    "        (\"cat\", categorical_pipeline, subset_features_2)  # Apply categorical_pipeline to second subset of features\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through other features not specified in subsets without any changes\n",
    ")\n",
    "\n",
    "# Define the complete model pipeline\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimator\", RandomForestRegressor(n_estimators=100, random_state=42))  # Using RandomForestRegressor as the model\n",
    "])\n",
    "\n",
    "# Example of how to fit the model (assuming X_train and y_train are already loaded)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Example of how to evaluate the model (assuming X_test and y_test are already loaded)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c2ff767-6c25-4f43-9dfb-14d9b37a0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.9097352406785437\n",
      "Mean of target variable: 40.693063173072595\n",
      "Median of target variable: 40.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(3.647088689489535)\n",
    "mean_target = y_train.mean()\n",
    "median_target = np.median(y_train)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean of target variable: {mean_target}')\n",
    "print(f'Median of target variable: {median_target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa9af918-0310-47fd-a41c-b1e216a535f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Changes in the Model Setup:\n",
    "\n",
    "1. Numerical Data Preprocessing:\n",
    "   - 'SimpleImputer' strategy changed from 'mean' to 'median':\n",
    "     This change helps handle outliers more effectively, as the median is less affected by extreme values compared to the mean.\n",
    "   - 'RobustScaler' replaces 'StandardScaler':\n",
    "     RobustScaler is used instead of StandardScaler to reduce the impact of outliers. It scales features using the median and the interquartile range, making it suitable for data with outliers.\n",
    "\n",
    "2. No changes in Categorical Data Preprocessing:\n",
    "   - The handling of categorical data remains the same with 'SimpleImputer' using 'most frequent' to fill missing values and 'OneHotEncoder' to handle categorical features.\n",
    "\n",
    "3. Model Pipeline:\n",
    "   - The overall structure of the model pipeline has not changed. It includes a preprocessor for handling both numerical and categorical data and uses 'RandomForestRegressor' as the estimator.\n",
    "   \n",
    "These adjustments make the model more robust to outliers and irregularities in the data, which is important for maintaining accuracy in real-world applications.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32aeba9f-c875-49d3-becc-040cd97f4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.7056169680228876\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing for numerical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # Use median for imputation to handle outliers\n",
    "    (\"scaler\", RobustScaler()),  # Use RobustScaler to reduce the influence of outliers\n",
    "    (\"pca\", PCA(n_components=3))  # Use PCA for dimensionality reduction\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical data\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing categories with the most frequent value\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown='ignore'))  # OneHotEncode categorical variables\n",
    "])\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, subset_features_1),\n",
    "        (\"cat\", categorical_pipeline, subset_features_2)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define the model pipeline\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimator\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model (assuming X_train, y_train are already loaded)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model (assuming X_test, y_test are already loaded)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "025eef00-c985-4308-b53b-25f5cc65de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.9249979137710482\n",
      "Mean of target variable: 40.693063173072595\n",
      "Median of target variable: 40.3\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(3.7056169680228876)\n",
    "mean_target = y_train.mean()\n",
    "median_target = np.median(y_train)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean of target variable: {mean_target}')\n",
    "print(f'Median of target variable: {median_target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "030c66c1-3a91-4dfc-9b1d-b6119e488172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Justifications for Using XGBoost in the Model Pipeline:\n",
    "\n",
    "1. Handling of Missing Values:\n",
    "   - XGBoost has an in-built capability to handle missing values automatically. This reduces the need for extensive data preprocessing and can lead to better handling of gaps in data.\n",
    "\n",
    "2. Regularization Features:\n",
    "   - XGBoost includes both L1 and L2 regularization which helps in reducing overfitting. Regularization is critical in complex models to ensure that they generalize well on unseen data.\n",
    "\n",
    "3. Model Flexibility:\n",
    "   - The ability to define custom objective functions and tuning a wide range of hyperparameters makes XGBoost highly adaptable. This flexibility allows it to be fine-tuned for specific dataset characteristics, such as the varying scales and distributions in sports performance data.\n",
    "\n",
    "4. Scalability and Efficiency:\n",
    "   - XGBoost is designed to be highly efficient and scalable, making it suitable for models that need to handle large volumes of data with complex relationships.\n",
    "\n",
    "5. Enhanced Tree Modeling:\n",
    "   - XGBoost uses a more sophisticated approach to building trees, which can model non-linear relationships more effectively than models like linear regression and can be more robust than random forests under certain configurations.\n",
    "\n",
    "Setting Parameters:\n",
    "   - 'n_estimators': Controls the number of boosting stages which should be increased or tuned based on the model performance and overfitting.\n",
    "   - 'learning_rate': Helps in controlling the contribution of each tree, making the model more robust by reducing the risk of overfitting.\n",
    "   - 'max_depth': Helps in managing the complexity of each tree, where deeper trees capture more detailed information.\n",
    "\n",
    "By leveraging these features, XGBoost can potentially offer superior performance on datasets with complex patterns and varied data quality, making it a strong candidate for comparison against other models.\n",
    "\"\"\"\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11aac3b-6146-4ccd-8a54-2e86678829f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for XGBoost: 2.9627747234977755\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing for numerical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values using median\n",
    "    (\"scaler\", StandardScaler())  # Scale features to zero mean and unit variance\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical data\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values with the most common category\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown='ignore'))  # OneHotEncode categorical variables\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, subset_features_1),\n",
    "        (\"cat\", categorical_pipeline, subset_features_2)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# XGBoost model configuration\n",
    "xgb_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f'Mean Squared Error for XGBoost: {mse_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ff35080-5545-4f28-9a73-8a41c674a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.721271252155736\n",
      "Mean of target variable: 40.693063173072595\n",
      "Median of target variable: 40.3\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(2.9627747234977755)\n",
    "mean_target = y_train.mean()\n",
    "median_target = np.median(y_train)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean of target variable: {mean_target}')\n",
    "print(f'Median of target variable: {median_target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7e68a38-fceb-40ac-b170-69deade1e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3dece1f-b209-4900-807e-26d36536302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2700\n",
      "[LightGBM] [Info] Number of data points in the train set: 22193, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 40.693063\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2616\n",
      "[LightGBM] [Info] Number of data points in the train set: 17754, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 40.949104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2637\n",
      "[LightGBM] [Info] Number of data points in the train set: 17754, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 40.339596\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2619\n",
      "[LightGBM] [Info] Number of data points in the train set: 17754, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 40.570970\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2657\n",
      "[LightGBM] [Info] Number of data points in the train set: 17755, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 40.805436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2637\n",
      "[LightGBM] [Info] Number of data points in the train set: 17755, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 40.800197\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Mean Squared Error: 2.7413961795929285\n"
     ]
    }
   ],
   "source": [
    "lgbm_params = {\n",
    "    'n_estimators': 200,  # Increase number of boosting rounds\n",
    "    'learning_rate': 0.05,  # Lower learning rate for more robust learning\n",
    "    'num_leaves': 31,  # Default, consider adjusting based on performance\n",
    "    'max_depth': -1,  # No limit, but consider setting a limit if overfitting occurs\n",
    "    'min_data_in_leaf': 20,  # Increase if model is overfitting\n",
    "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "    'objective': 'regression',\n",
    "    'random_state': 42  # For reproducibility\n",
    "}\n",
    "\n",
    "# Define features explicitly\n",
    "categorical_features = ['gender', 'nationality']\n",
    "numeric_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "# Preprocessing for numeric data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the models in the stack\n",
    "estimators = [\n",
    "    ('xgboost', XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=3, enable_categorical=True)),\n",
    "    ('lightgbm', LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)),\n",
    "    ('mlp', Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Ensure scaling is applied for MLP\n",
    "        ('mlp', MLPRegressor(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=500))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LinearRegression(),\n",
    "    passthrough=False,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Apply preprocessing and fit the model\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stack', stack_model)\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288597f-0bc2-407f-9dfb-ed32e3c9a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# model = model.fit(X_train, y_train)\n",
    "\n",
    "# # use your model to make predictions\n",
    "# yhat = model.predict(X_test)\n",
    "\n",
    "# # evaluate your code according to metrics by your choosing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5b342-f0f7-41b2-9bd0-a34598b990a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
